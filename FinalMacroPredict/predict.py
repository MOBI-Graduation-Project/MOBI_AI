# =================================================================
# â˜…â˜…â˜… v8 ëª¨ë¸ ì¼ì¼ ìë™ ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ (predict.py) â˜…â˜…â˜…
# â˜…â˜…â˜… (í”Œëœ B: CSV í´ë°± ê¸°ëŠ¥ íƒ‘ì¬) â˜…â˜…â˜…
# =================================================================
import pandas as pd
import numpy as np
import FinanceDataReader as fdr
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import time
import datetime
import os
import joblib 
from sqlalchemy import create_engine

import tensorflow as tf
from tensorflow.keras.models import load_model
from transformers import TFBertModel, BertTokenizer

print(f"[{datetime.datetime.now()}] ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘...")

# --- 2. ê¸°ë³¸ ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì • ---
BASE_PATH = "C:/MOBI_AI_TEST/" # (ì„œë²„ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)

# â˜…â˜…â˜…â˜…â˜… ìˆ˜ì •ì  1: v8 íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ë³€ê²½ â˜…â˜…â˜…â˜…â˜…
MODEL_PATH = os.path.join(BASE_PATH, "best_kospi_model_v8.keras")
SCALER_PATH = os.path.join(BASE_PATH, "kospi_scaler_v8.pkl")
# â˜…â˜…â˜…â˜…â˜…

CSV_NEWS_PATH = os.path.join(BASE_PATH, "news_headlines_security.csv") 
BERT_MODEL_NAME = "klue/bert-base" # v8ì€ klue/bert-baseë¡œ í•™ìŠµë¨ (v5ì™€ ë™ì¼)
DB_CONNECTION_STRING = "mysql+pymysql://USER:PASSWORD@HOST/DATABASE" # (ì„œë²„ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)


# --- 3. v8 ëª¨ë¸ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
# (v5ì™€ ì™„ë²½í•˜ê²Œ ë™ì¼í•©ë‹ˆë‹¤. ìˆ˜ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.)
def feature_engineer_live(df):
    df['MA5'] = df['Close'].rolling(window=5).mean()
    df['MA20'] = df['Close'].rolling(window=20).mean()
    df['V_MA5'] = df['Volume'].rolling(window=5).mean()
    df['V_MA20'] = df['Volume'].rolling(window=20).mean()
    df['Change_Ratio'] = (df['Close'] - df['Open']) / df['Open'] * 100
    return df

def get_latest_numeric_data():
    print("  [ë°ì´í„° ìˆ˜ì§‘] KOSPI ë° ê±°ì‹œ ê²½ì œ ì§€í‘œ ìˆ˜ì§‘ ì¤‘...")
    start_date = (datetime.date.today() - datetime.timedelta(days=60)).strftime('%Y-%m-%d')
    
    # 1. KOSPI ë°ì´í„° ë¡œë“œ (v8 í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼)
    kospi = fdr.DataReader('KS11', start_date)
    kospi_features = feature_engineer_live(kospi.copy())
    
    # 2. ê±°ì‹œ ê²½ì œ ì§€í‘œ ë¡œë“œ (v8 í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼)
    data_symbols = {
        'USD/KRW': 'USD/KRW', 'FRED:DGS10': 'US_10Y_BOND', 'GC=F': 'GOLD', 
        'CL=F': 'WTI_OIL', 'US500': 'SP500'
    }
    macro_features = []
    for symbol, name in data_symbols.items():
        df = fdr.DataReader(symbol, start_date)
        if 'Close' in df.columns: feature = df['Close'].rename(name)
        elif 'DGS10' in df.columns: feature = df['DGS10'].rename(name)
        else: feature = df.iloc[:, 0].rename(name)
        macro_features.append(feature)
    macro_df = pd.concat(macro_features, axis=1).ffill()
    
    # 3. KOSPIì™€ ê±°ì‹œ ì§€í‘œ ë³‘í•©
    final_numeric_df = pd.merge(kospi_features, macro_df, left_index=True, right_index=True, how='left').ffill()
    
    # 4. v8 ëª¨ë¸ì´ í•™ìŠµí•œ í”¼ì²˜ë§Œ ì •í™•íˆ ì„ íƒ (v8 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì˜ 16ê°œ í”¼ì²˜ ë¦¬ìŠ¤íŠ¸)
    v8_feature_columns = [
        'Open', 'High', 'Low', 'Close', 'Volume', 'Change', 
        'MA5', 'MA20', 'V_MA5', 'V_MA20', 'Change_Ratio',
        'USD/KRW', 'US_10Y_BOND', 'GOLD', 'WTI_OIL', 'SP500'
    ]
    # (ì´ ë¦¬ìŠ¤íŠ¸ëŠ” ì´ì œ v8 ìŠ¤ì¼€ì¼ëŸ¬ì™€ ì™„ë²½íˆ í˜¸í™˜ë©ë‹ˆë‹¤)
    final_numeric_df = final_numeric_df[v8_feature_columns].dropna()
    return final_numeric_df.iloc[[-1]]

def crawl_today_news():
    # (v5ì™€ ì™„ë²½í•˜ê²Œ ë™ì¼í•©ë‹ˆë‹¤. v8ì€ v5ì™€ ë™ì¼í•œ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.)
    print("  [ë°ì´í„° ìˆ˜ì§‘] ì˜¤ëŠ˜ ë‚ ì§œ 'ì¦ê¶Œ' ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘... (í”Œëœ A)")
    # ... (ë‚´ë¶€ ì½”ë“œ v5ì™€ ë™ì¼) ...
    base_url = "https://news.einfomax.co.kr/news/articleList.html"
    params = {'sc_section_code': 'S1N2', 'sc_order_by': 'E', 'page': 1}
    headers = {'User-Agent': 'Mozilla/5.0 ...'}
    today_str = datetime.date.today().strftime('%Y.%m.%d')
    today_headlines = []
    
    for page_num in range(1, 11): 
        try:
            response = requests.get(base_url, params=params, headers=headers, timeout=5) 
            response.raise_for_status() 
            soup = BeautifulSoup(response.text, 'html.parser')
            article_blocks = soup.select("ul.type1 > li")
            if not article_blocks: break 
            
            page_done = False
            for article in article_blocks:
                date_tag = article.select_one("em.info.dated")
                if date_tag:
                    date_text = date_tag.get_text(strip=True).split(' ')[0]
                    if date_text == today_str:
                        title = article.select_one("h4.titles > a").get_text(strip=True)
                        today_headlines.append(title)
                    elif date_text < today_str:
                        page_done = True 
                        break
            if page_done: break
            time.sleep(0.2)
        except Exception as e:
            print(f"  [í”Œëœ A ì—ëŸ¬] ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ë‹¨: {e}")
            return "" 
            
    return ' '.join(today_headlines)

def get_latest_news_from_csv():
    # (v5ì™€ ì™„ë²½í•˜ê²Œ ë™ì¼í•©ë‹ˆë‹¤.)
    print("  [Plan B] ì‹¤ì‹œê°„ í¬ë¡¤ë§ ì‹¤íŒ¨. CSVì—ì„œ ìµœì‹  ë‰´ìŠ¤ ë¡œë“œ ì¤‘...")
    # ... (ë‚´ë¶€ ì½”ë“œ v5ì™€ ë™ì¼) ...
    try:
        if not os.path.exists(CSV_NEWS_PATH):
            print(f"  [Plan B ì—ëŸ¬] {CSV_NEWS_PATH} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return ""
            
        df_security = pd.read_csv(CSV_NEWS_PATH)
        df_security['Date'] = pd.to_datetime(df_security['Date'], errors='coerce')
        df_security.dropna(subset=['Date', 'Title'], inplace=True)
        df_security.sort_values(by='Date', inplace=True)
        news_grouped = df_security.groupby('Date')['Title'].apply(lambda x: ' '.join(x)).reset_index()
        
        if not news_grouped.empty:
            latest_headlines = news_grouped.iloc[-1]['Title']
            latest_date = news_grouped.iloc[-1]['Date'].strftime('%Y-%m-%d')
            print(f"  [Plan B] CSVì˜ ê°€ì¥ ìµœì‹  ë‚ ì§œ({latest_date}) ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return latest_headlines
        else:
            print("  [Plan B ì—ëŸ¬] CSV íŒŒì¼ì— ìœ íš¨í•œ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""
    except Exception as e:
        print(f"  [Plan B ì—ëŸ¬] CSV ë¡œë“œ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        return ""

def save_to_db(prediction_result, model_accuracy):
    # (ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì´ í•¨ìˆ˜ ë‚´ë¶€ë¥¼ ì£¼ì„ ì²˜ë¦¬)
    pass 


# --- 4. ë©”ì¸ ì˜ˆì¸¡ ë¡œì§ ì‹¤í–‰ (v8ìš©) ---
def main():
    try:
        # 1. ëª¨ë¸ ë¡œë“œ (v8 íŒŒì¼ ë¡œë“œ)
        print("[1/5] v8 ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë”©...")
        model = load_model(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        
        # 2. ì „ë¬¸ê°€(BERT) ë¡œë“œ (v8ì´ ì‚¬ìš©í•œ klue/bert-base ë¡œë“œ)
        print("[2/5] KLUE-BERT ëª¨ë¸ ë¡œë”©...")
        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)
        bert_model = TFBertModel.from_pretrained(BERT_MODEL_NAME, from_pt=True)
        
        # 3. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ (v8 ë°ì´í„°ì™€ 100% í˜¸í™˜)
        print("[3/5] ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘...")
        today_numeric_df = get_latest_numeric_data()
        today_headlines_str = crawl_today_news() 
        
        is_plan_b = False
        if not today_headlines_str:
            print("  [ê²½ê³ ] ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨. Plan B (CSV)ë¥¼ ê°€ë™í•©ë‹ˆë‹¤.")
            today_headlines_str = get_latest_news_from_csv()
            is_plan_b = True
            
        if today_numeric_df.empty or not today_headlines_str:
            print("  [ìµœì¢… ê²½ê³ ] ìˆ«ì ë°ì´í„° ë˜ëŠ” Plan B ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì§€ ì•Šì•„ ì˜ˆì¸¡ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            return

        # 4. ë°ì´í„° ì „ì²˜ë¦¬
        print("[4/5] ë°ì´í„° ì „ì²˜ë¦¬...")
        # (v8 ìŠ¤ì¼€ì¼ëŸ¬ëŠ” ì´ì œ ì´ 16ê°œ í”¼ì²˜ì™€ ì™„ë²½íˆ í˜¸í™˜ë©ë‹ˆë‹¤)
        numeric_scaled = scaler.transform(today_numeric_df)
        bert_inputs = tokenizer(
            [today_headlines_str], max_length=128, truncation=True, 
            padding='max_length', return_tensors='tf'
        )
        bert_outputs = bert_model(bert_inputs)
        news_features = bert_outputs.last_hidden_state[:, 0, :].numpy()
        
        # 5. ì˜ˆì¸¡ ìˆ˜í–‰ ë° ì €ì¥
        print("[5/5] ì˜ˆì¸¡ ìˆ˜í–‰ ë° DB ì €ì¥...")
        pred_prob = model.predict({'numeric_input': numeric_scaled, 'news_input': news_features})
        prediction_result = "ìƒìŠ¹ğŸ“ˆ" if pred_prob[0][0] > 0.5 else "í•˜ë½ğŸ“‰"
        
        # â˜…â˜…â˜…â˜…â˜… ìˆ˜ì •ì  2: v8ì˜ ì •í™•ë„ë¡œ ë³€ê²½ â˜…â˜…â˜…â˜…â˜…
        # (Colabì—ì„œ v8 í•™ìŠµ í›„ ë‚˜ì˜¨ ìµœì¢… ì •í™•ë„(ì˜ˆ: 53.40)ë¥¼ ì—¬ê¸°ì— ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤)
        model_accuracy = 53.40 # ì˜ˆì‹œì…ë‹ˆë‹¤. Colab ê²°ê³¼ê°’ìœ¼ë¡œ ê¼­ ìˆ˜ì •í•˜ì„¸ìš”!
        # â˜…â˜…â˜…â˜…â˜…
        
        if is_plan_b:
             print(f"  [ì˜ˆì¸¡ ì™„ë£Œ - Plan B] ë‚´ì¼ KOSPI ì˜ˆì¸¡: {prediction_result} (Prob: {pred_prob[0][0]:.4f})")
             print("  [ì£¼ì˜] ì´ ì˜ˆì¸¡ì€ ì‹¤ì‹œê°„ ë‰´ìŠ¤ê°€ ì•„ë‹Œ, ì €ì¥ëœ ìµœì‹  ë‰´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.")
        else:
             print(f"  [ì˜ˆì¸¡ ì™„ë£Œ - Plan A] ë‚´ì¼ KOSPI ì˜ˆì¸¡: {prediction_result} (Prob: {pred_prob[0][0]:.4f})")
        
        # DBì— ì €ì¥
        # save_to_db(prediction_result, model_accuracy)
        print("\nâ˜…â˜…â˜… ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ! DB ì €ì¥ì„ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤. â˜…â˜…â˜…") # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©

    except Exception as e:
        print(f"[{datetime.datetime.now()}] ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì ì¸ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
    print(f"[{datetime.datetime.now()}] ì˜ˆì¸¡ ìŠ¤í¬ë¦½íŠ¸ ì¢…ë£Œ.")